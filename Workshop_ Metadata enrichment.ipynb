{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Workshop: Unstructured Data.ipynb","version":"0.3.2","provenance":[{"file_id":"1Gd6OO9PYjRLK2G0Ee5hLZRp7xQaMrANP","timestamp":1541077923977}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"O0Q8i9RhnBNf","colab_type":"text"},"cell_type":"markdown","source":["<h2>Metadata generation for music reviews using Googleâ€™s Natural Language API</h2>\n","\n","---\n","\n","This workshop will introduce you to accessing Google Cloud resources through the command line and python client libraries.\n","\n","First, you will load in some review data from a Cloud Storage bucket and explore what it looks like. Then, connect to the Cloud Language AI through the python client library and see how each API responds.\n","\n","Finally, apply the API to the reviews, and examine the added value of the results you get. If you like data exploration, spend some time cross-correlating these results with the other structured data found in the dataset. Or think about how the API has opened up new possibilities for further enriching this dataset."]},{"metadata":{"id":"PXqokZmgnBNg","colab_type":"text"},"cell_type":"markdown","source":["<h3>Setting up access</h3>"]},{"metadata":{"id":"9bHyFi9JT--L","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import auth\n","\n","auth.authenticate_user()\n","\n","!gsutil cp gs://datatonic-pitchfork-data/datatonic-external-training-24407c914082.json ."],"execution_count":0,"outputs":[]},{"metadata":{"id":"1Nua7i9L-0T-","colab_type":"text"},"cell_type":"markdown","source":["<h3>Importing Libraries</h3>"]},{"metadata":{"id":"NvKv32_snBNh","colab_type":"code","colab":{}},"cell_type":"code","source":["# update to latest version\n","!pip install --upgrade google-cloud-language"],"execution_count":0,"outputs":[]},{"metadata":{"id":"scfNYluqnBNo","colab_type":"code","colab":{}},"cell_type":"code","source":["# import libraries\n","import pandas as pd\n","import glob\n","import functools\n","from google.cloud import language\n","from google.cloud.language import enums\n","from google.cloud.language import types\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","colours = [\"#1637b5\", \"#79808c\", \"#1dbbf8\", \"#9e51da\", \"#f4454f\"]\n","sns.set_palette(colours)\n","sns.set(font=\"Arial\")\n","sns.set_style(\"whitegrid\")\n","%matplotlib inline\n","\n","# instantiate a client\n","language_client = language.LanguageServiceClient.from_service_account_file(\"./datatonic-external-training-24407c914082.json\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3DQ9dclR_B_6","colab_type":"text"},"cell_type":"markdown","source":["Helper functions"]},{"metadata":{"id":"TUoIU5a-_GGW","colab_type":"code","colab":{}},"cell_type":"code","source":["def show_sentiments(annotations, n):\n","    score = annotations.document_sentiment.score\n","    magnitude = annotations.document_sentiment.magnitude\n","\n","    for index, sentence in enumerate(annotations.sentences[:n]):\n","        sentence_sentiment = sentence.sentiment.score\n","        print('Sentence {} has a sentiment score of {:.3f}'.format(\n","            index + 1, sentence_sentiment))\n","        print('Sentence text:\\n{}\\n'.format(sentence.text.content))\n","\n","    print('Overall Sentiment: score of {:.3f} with magnitude of {:.3f}'.format(\n","        score, magnitude))\n","    return 0\n","\n","\n","def show_categories(categories, n):\n","    for category in categories.categories[:n]:\n","        print('=' * 20)\n","        print('name: {}'.format(category.name))\n","        print('confidence: {:.3f}'.format(category.confidence))\n","        \n","        \n","def show_sentences(syntax, n):\n","    for sentence in syntax.sentences[:n]:\n","        print('=' * 20)\n","        print('{}'.format(sentence.text.content))\n","        \n","\n","def show_tokens(syntax, n):\n","    for token in syntax.tokens[:n]:\n","        print('=' * 20)\n","        print('text:  {}'.format(token.text.content))\n","        print('part_of_speech:\\n{}'.format('  ' + str(token.part_of_speech).replace('\\n', '\\n  ')))\n","        print('lemma: {}'.format(token.lemma))\n","        \n","\n","def show_entities(entities, n):\n","    for entity in entities.entities[:n]:\n","        print('=' * 20)\n","        print('         name: {0}'.format(entity.name))\n","        print('         type: {0}'.format(entity.Type.Name(entity.type)))\n","        print('     metadata: {0}'.format(dict(entity.metadata)))\n","        print('     salience: {:.4f}'.format(entity.salience))\n","        \n","        \n","def show_entity_sentiments(entity_sentiments, n):\n","    for entity in entity_sentiments.entities[:n]:\n","        print('=' * 20)\n","        print('         name: {0}'.format(entity.name))\n","        print('         type: {0}'.format(entity.Type.Name(entity.type)))\n","        print('        score: {:.3f}'.format(entity.sentiment.score))\n","        print('    magnitude: {:.3f}'.format(entity.sentiment.magnitude))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZrHU6erDnBNs","colab_type":"text"},"cell_type":"markdown","source":["<h3>Downloading Dataset</h3>"]},{"metadata":{"id":"OoxtaILznBNt","colab_type":"text"},"cell_type":"markdown","source":["Download the pitchfork reviews dataset from Google Cloud Storage. "]},{"metadata":{"id":"d8TaDU3dnBNu","colab_type":"code","colab":{}},"cell_type":"code","source":["!gsutil -m cp -r gs://datatonic-pitchfork-data/*.csv ."],"execution_count":0,"outputs":[]},{"metadata":{"id":"L_1R8F6jnBNw","colab_type":"code","colab":{}},"cell_type":"code","source":["# check that the files are present in current directory\n","!ls *.csv"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KHCJ78M3nBNz","colab_type":"text"},"cell_type":"markdown","source":["Read the CSV files into pandas DataFrames."]},{"metadata":{"id":"YJ4XbeV-nBNz","colab_type":"code","colab":{}},"cell_type":"code","source":["content = pd.read_csv('content.csv', header=0, delimiter='|')\n","genres = pd.read_csv('genres.csv', header=0, delimiter='|')\n","labels = pd.read_csv('labels.csv', header=0, delimiter='|')\n","reviews = pd.read_csv('reviews.csv', header=0, delimiter='|')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"an3Z8kPnnBN3","colab_type":"text"},"cell_type":"markdown","source":["<h3>Exploring the dataset</h3>"]},{"metadata":{"id":"FUWTphVlnBN5","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"Content has {} rows and {} columns.\".format(*content.shape))\n","print(\"Genres has {} rows and {} columns.\".format(*genres.shape))\n","print(\"Labels has {} rows and {} columns.\".format(*labels.shape))\n","print(\"Reviews has {} rows and {} columns.\".format(*reviews.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3kLx-Wya_vmi","colab_type":"code","colab":{}},"cell_type":"code","source":["reviews.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PIuP3Qmg_4Rk","colab_type":"text"},"cell_type":"markdown","source":["Genres dataset's first 5 rows:"]},{"metadata":{"id":"LxEtTLpf_w4t","colab_type":"code","colab":{}},"cell_type":"code","source":["genres.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KP5nSmYS_71o","colab_type":"text"},"cell_type":"markdown","source":["Labels dataset's first 5 rows:"]},{"metadata":{"id":"_QnYagzO_1_B","colab_type":"code","colab":{}},"cell_type":"code","source":["labels.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1-ab7wKEAAJk","colab_type":"text"},"cell_type":"markdown","source":["Content dataset's first 5 rows:"]},{"metadata":{"id":"7NK6uzjc_9WP","colab_type":"code","colab":{}},"cell_type":"code","source":["content.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8xQyGA4WADr2","colab_type":"text"},"cell_type":"markdown","source":["We might want to join up the datasets on reviewId later on to get a better idea of the context for each review, but for now, let's just use the content dataset and try inputting the free review text as is into Google's natural language API and see what insights we can get immediately.\n","\n","Now, let's pick a review text and explore the Natural Language API. I've picked a review about The Beatle's Let It Be album. You are welcome to search for your favourite band/artist on https://pitchfork.com and copy the review ID from the URL. For example, for The Beatles, the URL is https://pitchfork.com/reviews/albums/13430-let-it-be/ and the review ID is 13430."]},{"metadata":{"id":"3na-dBof_--V","colab_type":"code","colab":{}},"cell_type":"code","source":["review_id = 13430\n","review_text = row = content[content.reviewid == review_id]['content'].values[0]\n","print('{}...'.format(review_text[:205]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"du7z8QD1AFYb","colab_type":"code","colab":{}},"cell_type":"code","source":["document = types.Document(content=review_text, type=enums.Document.Type.PLAIN_TEXT)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jPAQu9lIAI6S","colab_type":"text"},"cell_type":"markdown","source":["<h3>Content classification</h3>\n","\n","<p>The Natural Language API can be used for analyzing documents and obtaining lists of content categories that apply to the text in the document.</p>"]},{"metadata":{"id":"aSuXSwxnAGsO","colab_type":"code","colab":{}},"cell_type":"code","source":["categories = language_client.classify_text(document=document)\n","\n","show_categories(categories, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YX1CjdaxNe0N","colab_type":"text"},"cell_type":"markdown","source":["<br />\n","<p>The API managed to pick up the broad categories that the review falls under. For The Beatles' album, the categories are \"Arts & Entertainment\", \"Music & Audio\" and \"Rock Music\". Depending on which review you picked, you may see different categories. If you want to explore the API's capabilities further, there is a <a href=\"https://cloud.google.com/natural-language/\">free demo</a> (scroll down a bit to \"Try the API\") that you can try. See whether the API can classify your text correctly if you describe an animal or an object without using the word for it! Below is the text I experimented with.</p>\n","\n","<blockquote>\n","<p>These animals are domesticated felines of the genus felidae who are smaller than the majority of their wild cousins who share the genus. Efficient hunters with speed, sharp teeth, retractable claws, superior hearing, eyes which can function in near-complete darkness and a wide variety of vocalizations. Primarily short-haired, but with long-haired variations resulting from selective breeding. Markings can vary from a single solid color to various stripes, swirls, spots and differently-colored extremities, primarily in tones of brown, black, orange, red, white and grey.</p>\n","\n","<p>They are popular as pets and generally considered sleek, beautiful, intelligent, fastidious and aloof, but are also known for their independence,  playfulness and the purr, a vocalization that can indicate contentedness, security and affection, as well as distress and self-comfort. Humans find the purr comforting and studies have shown that the purr reduces stress in humans.</p>\n","</blockquote>\n","\n","<p>See whether you can find out which words contribute the most to the API's confidence that the category \"/Pets & Animals/Pets/Cats\" is a category it should associate with the text."]},{"metadata":{"id":"vs2fRd_6NyyC","colab_type":"text"},"cell_type":"markdown","source":["<h3>Syntactic analysis</h3>\n","<p>The Natural Language API provides a powerful set of tools for analyzing and parsing text through syntactic analysis. Syntactic analysis consists of <b>sentence extraction</b> (breaks up the stream of text into a series of sentences) and <b>tokenization</b> (breaks up the stream of text into a series of tokens where each token corresponds to a single word).</p>"]},{"metadata":{"id":"_o9_yruTANE1","colab_type":"code","colab":{}},"cell_type":"code","source":["syntax = language_client.analyze_syntax(document)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jvF2eHRzN3Et","colab_type":"text"},"cell_type":"markdown","source":["<b>First 5 sentences</b>"]},{"metadata":{"id":"qO4v02aHN08Y","colab_type":"code","colab":{}},"cell_type":"code","source":["show_sentences(syntax, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3ISoj8PiN6V0","colab_type":"text"},"cell_type":"markdown","source":["<b>First 5 tokens</b>"]},{"metadata":{"id":"vgcnz_JvN4Yf","colab_type":"code","colab":{}},"cell_type":"code","source":["show_tokens(syntax, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uvPPZ3AjN-Gh","colab_type":"text"},"cell_type":"markdown","source":["<p>The <code>text</code> field contains the text data associated with the token. The <code>part_of_speech</code> field provides grammatical information including <a href=\"https://en.wikipedia.org/wiki/Morphology_(linguistics)\">morphological information</a>, about the token, such as the token's tense, person, number, gender, etc (for more information, refer to the <a href=\"https://cloud.google.com/natural-language/docs/morphology\">documentation</a>). The <code>lemma</code> field contains the \"root\" word upon which this word is based, which allows you to canonicalize word usage within your text. For example, the words \"write\", \"writing\", \"wrote\" and \"written\" are all based on the same lemma (\"write\"). As well, plural and singular forms are based on lemmas: \"house\" and \"houses\" both refer to the same form.</p>\n","\n","<p>There are also other more advanced fields available. For more information, refer to the <a href=\"https://cloud.google.com/natural-language/docs/\">Natural Language API documentation</a>.</p>\n","\n","<h3>Sentiment analysis</h3>"]},{"metadata":{"id":"dfWKm5h_N7yE","colab_type":"code","colab":{}},"cell_type":"code","source":["sentiments = language_client.analyze_sentiment(document=document)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xCedNi1oOC6y","colab_type":"text"},"cell_type":"markdown","source":["The sentiments for the first 5 sentences are:"]},{"metadata":{"id":"kor1kM45OAux","colab_type":"code","colab":{}},"cell_type":"code","source":["show_sentiments(sentiments, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"URgWsVfjO9KF","colab_type":"text"},"cell_type":"markdown","source":["<p>We can see that it is really easy to get started and obtain meaningful results very quickly. The language API picked up the text language automatically and preprocessed it without us having to do any cleaning.</p>\n","\n","<h4>Guide to interpreting sentiment analysis values</h4>\n","\n","<p>The <i>score</i> of a document's sentiment indicates the overall emotion of a document. The <i>magnitude</i> of a document's sentiment indicates how much emotional content is present within the document, and this value is often proportional to the length of the document.</p>\n","\n","<p>It is important to note that the Natural Language API indicates differences between positive and negative emotion in a document, but does not identify specific positive and negative emotions. For example, \"angry\" and \"sad\" are both considered negative emotions. However, when the Natural Language API analyzes text that is considered \"angry\", or text that is considered \"sad\", the response only indicates that the sentiment in the text is negative, not \"sad\" or \"angry\".</p>\n","\n","<p>A document with a neutral score (around <code>0.0</code>) may indicate a low-emotion document, or may indicate mixed emotions, with both high positive and negative values which cancel each out. Generally, you can use <code>magnitude</code> values to disambiguate these cases, as truly neutral documents will have a low <code>magnitude</code> value, while mixed documents will have higher magnitude values.</p>\n","\n","<p>When comparing documents to each other (especially documents of different length), make sure to use the <code>magnitude</code> values to calibrate your scores, as they can help you gauge the relevant amount of emotional content.</p>\n","\n","<p>The chart below shows some sample values and how to interpret them:</p>\n","\n","| Sentiment      | Sample Values |\n","| -------------- | ------------- |\n","| Clearly Positive      | \"<b>score</b>\": 0.8, \"<b>magnitude</b>\": 3.0       |\n","| Clearly Negative | \"<b>score</b>\": -0.6, \"<b>magnitude</b>\": 4.0 |\n","| Neutral | \"<b>score</b>\": 0.1, \"<b>magnitude</b>\": 0.0 |\n","| Mixed | \"<b>score</b>\": 0.0, \"<b>magnitude</b>\": 4.0 |\n","\n","<h3>Entity analysis</h3>\n","<p>We can also perform entity analysis on our review text. Entity analysis provides information about entities in the text, which generally refer to named \"things\" such as famous individuals, landmarks, common objects, etc.</p>"]},{"metadata":{"id":"be9x3YFkOFcX","colab_type":"code","colab":{}},"cell_type":"code","source":["entities = language_client.analyze_entities(document=document)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FpSNP6pRPESV","colab_type":"text"},"cell_type":"markdown","source":["The first 5 entities with their names, types, metadata and salience is shown below."]},{"metadata":{"id":"VadGNrgFPC4f","colab_type":"code","colab":{}},"cell_type":"code","source":["show_entities(entities, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g2JjVY4ePJYA","colab_type":"text"},"cell_type":"markdown","source":["<p>The <code>type</code> field indicates the type of the entity (e.g. person, location, event, etc). This information helps distinguish and/or disambiguate entities, and can be used for writing patterns or extracting information. For example, a <code>type</code> value can help distinguish similarly named entities such as \"Lawrence of Arabia\", tagged as a <code>WORK_OF_ART</code> (film), from \"T.E Lawrence\", tagged as a <code>PERSON</code>, for example.</p>\n","\n","<p>The <code>metadata</code> field contains source information about the entity's knowledge repository and may contain <code>wikipedia_url</code> and <code>mid</code> (machine-generated identifier corresponding to the entity's <a href=\"https://www.google.com/intl/bn/insidesearch/features/search/knowledge.html\">Google Knowledge Graph</a> entry which remains unique across languages and can be used to tie entities together from different languages).</p>\n","\n","<p>The <code>salience</code> field indicates the importance or relevance of this entity to the entire document text. This score can assist information retrieval and summarization by prioritizing salient entities. Scores closer to <code>0.0</code> are less important, while scores closer to <code>1.0</code> are highly important</p>\n","\n","<h3>Entity sentiment analysis</h3>\n","<p>We can also combine named entities and sentiment analysis and obtain sentiments for each named entity.</p>"]},{"metadata":{"id":"DPpOA__aPFbh","colab_type":"code","colab":{}},"cell_type":"code","source":["entity_sentiments = language_client.analyze_entity_sentiment(document=document)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OsIlthqbPMOV","colab_type":"text"},"cell_type":"markdown","source":["The first 5 entity sentiments with their names, types, scores and magnitude is shown below."]},{"metadata":{"id":"8jaQiioXPLEb","colab_type":"code","colab":{}},"cell_type":"code","source":["show_entity_sentiments(entity_sentiments, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vm4m2b-yPRJp","colab_type":"text"},"cell_type":"markdown","source":["<h3>Extracting structured information</h3>\n","\n","<p>Now that we've explored the Natural Language API, we can start thinking about how to extract more structured information from the text. We may need it to conform to a schema or we may need to create additional features for a machine learning model. The set of features of interest will vary depending on the use case, but for the purposes of this exercise, let's assume we want to be able to predict the review score. The overall sentiment of the text and the categories it falls under could make good features for this task. Let's join up the data and add the additional features.</p>\n","\n","<p>First, let's merge <code>content</code> with <code>reviews</code> because they are one-to-one.</p>"]},{"metadata":{"id":"zjfKdlQCPNdc","colab_type":"code","colab":{}},"cell_type":"code","source":["df = pd.merge(content.dropna(), reviews.dropna(), how='inner', on='reviewid')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qXJ3TcfTPhkO","colab_type":"text"},"cell_type":"markdown","source":["Next, let's group the labels per review and list them as comma separated values, so that we can join them to the dataframe from the previous step."]},{"metadata":{"id":"qX709M42PQd7","colab_type":"code","colab":{}},"cell_type":"code","source":["review_labels = labels.dropna().groupby(['reviewid'])['label'].apply(', '.join).reset_index()\n","df = pd.merge(df, review_labels, how='inner', on='reviewid')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kS-6saaYPkRk","colab_type":"text"},"cell_type":"markdown","source":["Finally, let's join do the same for <code>genres</code>."]},{"metadata":{"id":"BD-BgGjLPiwp","colab_type":"code","colab":{}},"cell_type":"code","source":["review_genres = genres.dropna().groupby(['reviewid'])['genre'].apply(', '.join).reset_index()\n","df = pd.merge(df, review_genres, how='inner', on='reviewid')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hKfmSGnjPsgn","colab_type":"text"},"cell_type":"markdown","source":["Great, we have all the review information in one dataframe now. Let's have a peek."]},{"metadata":{"id":"HWlWhfTpPqGa","colab_type":"code","colab":{}},"cell_type":"code","source":["df.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f6Q1m1XKPwgD","colab_type":"text"},"cell_type":"markdown","source":["<p>Now, let's augment the dataframe above with some additional features. In particular, the sentiment of the text and the categories the text falls under would make useful features. We can also add comma separated list of important entities in the text as a feature. Let's get started.</p>\n","\n","<p>Running classification on the entire dataframe might take a while and we might run into API rate limiting issues, so selecting a small subset of the data is a good way to demonstrate the process without spending too long on waiting for requests to complete.</p>\n","\n","<p>Let's create a mini dataframe with just the first 10 rows.</p>"]},{"metadata":{"id":"8Me3Z0AwPuOo","colab_type":"code","colab":{}},"cell_type":"code","source":["mini_df = df[:10]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vG_WzeRgPzra","colab_type":"text"},"cell_type":"markdown","source":["Then, let's run classification on the text for each one and add it to <code>mini_df</code> as a column."]},{"metadata":{"id":"iyfuykQPPyNH","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_category(content):\n","    document = types.Document(content=content, type=enums.Document.Type.PLAIN_TEXT)\n","    categories = language_client.classify_text(document=document)\n","    return ', '.join([c.name for c in categories.categories])\n","\n","mini_df_categories = mini_df.content.apply(get_category)\n","mini_df['categories'] = mini_df_categories"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nbd0Po0HP06V","colab_type":"code","colab":{}},"cell_type":"code","source":["mini_df.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g_W_d03XP5Wt","colab_type":"text"},"cell_type":"markdown","source":["Next, let's obtain the sentiment scores and magnitudes for each review and add that as a column too."]},{"metadata":{"id":"3iQM4LlKP2E6","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_sentiment_scores(content):\n","    document = types.Document(content=content, type=enums.Document.Type.PLAIN_TEXT)\n","    sentiments = language_client.analyze_sentiment(document=document)\n","    score = sentiments.document_sentiment.score\n","    magnitude = sentiments.document_sentiment.magnitude\n","    return pd.Series([score, magnitude])\n","\n","mini_df_sentiment_scores = mini_df.content.apply(get_sentiment_scores)\n","mini_df_sentiment_scores = pd.DataFrame(mini_df_sentiment_scores)\n","mini_df_sentiment_scores.columns = ['score', 'magnitude']\n","mini_df = pd.concat([mini_df, mini_df_sentiment_scores], axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qc7EdbI1QHKf","colab_type":"text"},"cell_type":"markdown","source":["Finally, let's add an <code>entities</code> column which includes the 5 entities with the highiest salience scores."]},{"metadata":{"id":"iXd29nXfP7x3","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_top_entities(content):\n","    document = types.Document(content=content, type=enums.Document.Type.PLAIN_TEXT)\n","    entities = language_client.analyze_entities(document=document)\n","    return ', '.join([e.name for e in entities.entities[:5]])\n","\n","top_entities = mini_df.content.apply(get_top_entities)\n","mini_df['entities'] = top_entities"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dbuD7OY4QIr1","colab_type":"code","colab":{}},"cell_type":"code","source":["mini_df.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c3ejIvY9QW7W","colab_type":"text"},"cell_type":"markdown","source":["<p>That's it! We have added a few additional features which a machine learning model trying to predict the review score can use. These features are much easier to encode and feed into a model than free-form text is. The Natural Language API has done most of the heavy lifting for us.</p>\n","\n","<p>Feel free to experiment and see what other features can be extracted using the APIs!</p> "]},{"metadata":{"id":"TaK-klj6QZGy","colab_type":"text"},"cell_type":"markdown","source":["<h3>Using spaCy</h3>\n","\n","<p>For natural language processing, spaCy is a well regarded free python library which supports similar linguistic features to the Natural Language API. You can read more about it <a href=\"https://spacy.io/usage/linguistic-features\">here</a>. In this tutorial, we won't explore spaCy in detail, but it is good to know that an open source alternative is available</p>\n","\n","<p><b>spaCy supports:</b></p>\n","<ol>\n","    <li><a href=\"https://spacy.io/usage/linguistic-features#section-pos-tagging\">Part-Of-Speech Tagging</a></li>\n","    <li><a href=\"https://spacy.io/usage/linguistic-features#section-dependency-parse\">Dependency Parse</a></li>\n","    <li><a href=\"https://spacy.io/usage/linguistic-features#section-named-entities\">Named Entities</a></li>\n","    <li><a href=\"https://spacy.io/usage/linguistic-features#section-tokenization\">Tokenization</a></li>\n","    <li><a href=\"https://spacy.io/usage/linguistic-features#section-sbd\">Sentence Segmentation</a></li>\n","    <li><a href=\"https://spacy.io/usage/linguistic-features#section-rule-based-matching\">Rule-based Matching</a></li>\n","</ol>\n","\n","<h3>Data exploration</h3>\n","\n","<p>For the remainder of the tutorial we will be exploring the data a bit more and trying to answer some of the questions below.</p>\n","\n","<h4>Exploratory Analysis</h4>\n","\n","Let us answer some of the questions below to gain a better understanding of the dataset, especially now that it is merged:\n","\n","* How many distinct artists/albums/genres/record labels do we have in our dataset?\n","* What is the trend of reviews posted over time? When were the most reviews posted, and what are the most common times?\n","* Who are the most popular artists that are reviewed?\n","* What are the most popular albums/genres/record labels to write about?\n","* Who are the authors that have contributed the most?\n","* Which artists/albums/genres/record labels have the highest/lowest average scores?\n","* Are some authors more biased to giving high/low scores?\n","* How does the author type influence the score?"]},{"metadata":{"id":"uJ9DHKvQQKkW","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"The time range of the dataset spans {} to {}\".format(df[\"pub_date\"].min(), df[\"pub_date\"].max()))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nUYaqqmaQgts","colab_type":"code","colab":{}},"cell_type":"code","source":["# how many unique values across categories of interest?\n","print(\"Number of unique authors: {}\".format(df[\"author\"].nunique()))\n","print(\"Number of unique artists: {}\".format(df[\"artist\"].nunique()))\n","print(\"Number of unique albums: {}\".format(df[\"title\"].nunique()))\n","print(\"Number of unique genres: {}\".format(df[\"genre\"].nunique()))\n","print(\"Number of unique record labels: {}\".format(df[\"label\"].nunique()))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bp4DOwCVQhuK","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot(df, group_col, type=\"bar\", xlab=None, ylab=None, title=None, fig_size= (8,4)):\n","    trend = df.groupby([group_col]).size().reset_index()\n","    trend.columns = [group_col, \"count\"]\n","    plt.figure(figsize=fig_size)\n","  \n","    if type == \"ts\":\n","        ax = sns.tsplot(trend[\"count\"], trend[group_col], color=colours[0])\n","    elif type == \"bar\":\n","        trend = trend.sort_values(by=\"count\", ascending=False)\n","        ax = sns.barplot(y=trend[group_col], x=trend[\"count\"], color=colours[1])\n","    ax.set(xlabel=xlab, ylabel=ylab, title=title)\n","    plt.show()\n","\n","plot(df, \"pub_year\", \"ts\", \"Publish Year\", \"Number of Reviews\", \"Yearly Trend of Reviews\")\n","plot(df, \"pub_month\", \"ts\", \"Publish Month\", \"Number of Reviews\", \"Monthly Trend of Reviews\")\n","plot(df, \"pub_weekday\", \"ts\", \"Publish Weekday\", \"Number of Reviews\", \"Day of Week Trend of Reviews\")\n","print(\"Note that 0 corresponds to Monday, and 6 corresponds to Sunday.\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3bSDu-BGQkvn","colab_type":"text"},"cell_type":"markdown","source":["* The number of reviews was the greatest in <b>2016</b> - in general it has been increasing year on year and so we can assume the number of reviews in 2017 exceeded 2016. The biggest jump in absolute volume (of 467 reviews) was from 2001 to 2002.\n","* The most reviews are published in the month of <b>October</b> (across all years) with a total of 1763 reviews, however in December it drops significantly to 819 which is expected as it is around Christmas time and there are also probably less albums released around that time.\n","* The most reviews are published on <b>Tuesday</b> which makes sense because prior to July 2015, new music was released on Tuesdays in the US, and then this was changed to Fridays. Note that Pitchfork is an American website."]},{"metadata":{"id":"lPC1NwkhQisc","colab_type":"code","colab":{}},"cell_type":"code","source":["# total reviews per genre\n","plot(genres, \"genre\", \"bar\",\"Number of Reviews\", \"Genre\", \"Total Reviews by Genre\", (10, 6))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FN2ensaGQnvr","colab_type":"text"},"cell_type":"markdown","source":["<p>The volume of reviews for the <b>rock genre</b> are at least 2x more than any other genre.</p>\n","\n","<p>Let's see which artists were the popular for each year. Where there is a tie, I've taken the top 3 artists.</p>"]},{"metadata":{"id":"q9Wk1HQuQl51","colab_type":"code","colab":{}},"cell_type":"code","source":["artist_reviews_per_year = df.groupby(['pub_year','artist']).size().reset_index()\n","artist_reviews_per_year.columns = ['pub_year', 'artist', 'num_reviews']\n","artist_reviews_per_year = artist_reviews_per_year.sort_values(by=['pub_year', 'num_reviews'], ascending=False)\n","\n","most_popular_artists_per_year = []\n","\n","for name, group in artist_reviews_per_year.groupby(['pub_year']):\n","    max_count, artists, n = -1, [], 3\n","    for row, data in group.iterrows():\n","        if data['num_reviews'] > max_count:\n","            max_count = data['num_reviews']\n","            artists = [data['artist']]\n","            continue\n","\n","        if data['num_reviews'] == max_count:\n","            artists.append(data['artist'])\n","            n -= 1\n","            \n","        if n == 0 or data['num_reviews'] < max_count:\n","            most_popular_artists_per_year.append([name, ', '.join(artists), max_count])\n","            break\n","        \n","most_popular_artists_per_year_df = pd.DataFrame(most_popular_artists_per_year)\n","most_popular_artists_per_year_df.columns = ['pub_year', 'artist', 'num_reviews']\n","most_popular_artists_per_year_df.sort_values(by=['pub_year'], ascending=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n_lgtqRkQrA3","colab_type":"text"},"cell_type":"markdown","source":["See whether you can answer the remaining questions that were posed in the beginning by exploring the data further!"]},{"metadata":{"id":"tPvTohYKQpDr","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}